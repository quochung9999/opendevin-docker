services:
  opendevin:
    build: ./
    container_name: opendevin
    privileged: true
    runtime: nvidia
    tty: true
    ulimits:
      memlock: -1
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,video
    ports:
      - "80:80"
      - "3000:3000"
      - "3001:3001"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
      - "./nginx.conf:/etc/nginx/nginx.conf:ro"
    networks:
      - opendevin

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: webui
    depends_on:
      - ollama
    ports:
      - 8080:8080
    environment:
      - "OLLAMA_BASE_URL=http://ollama:11434"
      - 'WEBUI_SECRET_KEY='
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - opendevin
    #volumes:
      #- /mnt/ssd/ai/open-webui:/app/backend/data

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    privileged: true
    pull_policy: always
    runtime: nvidia
    tty: true
    ulimits:
      memlock: -1
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,video
    ports:
      - 11434:11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - opendevin
    #volumes:
      #- /mnt/ssd/ai/ollama:/root/.ollama

networks:
  opendevin:
    external: true
    name: opendevin
